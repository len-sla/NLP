{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbQo5QPSWTLU"
   },
   "source": [
    "This notebook shows how you can use Google Colab with free GPU to get Bert model predictions.\n",
    "Generally it is nothing special except the times where there is not enough GPU memory to do the task.\n",
    "To make it possible traing set is being traine in batches. Below there is example  BERT model = 'cased_L-12_H-768_A-12' is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cW5QmuhUMxZ"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo-umXqyVDy6"
   },
   "source": [
    "First you need to pass details of the bert model that you want to use.\n",
    "You can get these parameters from [this website](https://github.com/google-research/bert). When you right-click on the selected model, you should be able to copy the link which has information about `date` and `name` of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxeY_4rBWD9w"
   },
   "outputs": [],
   "source": [
    "bert_model_name = 'cased_L-12_H-768_A-12'\n",
    "bert_model_date = '2018_10_18'\n",
    "bert_model_path = f'https://storage.googleapis.com/bert_models/{bert_model_date}/{bert_model_name}.zip'\n",
    "uncased=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKAhfA-cVwc2"
   },
   "source": [
    "You need to specify directories that will be used in the notebook:\n",
    "- directory with model data,\n",
    "- directory with input data,\n",
    "- output directory.\n",
    "\n",
    "Path `./drive/My Drive/` points to your Google Drive where you can create folder to be used for storing input and output data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKjR19LvUwj8"
   },
   "outputs": [],
   "source": [
    "model_dir = f'./{bert_model_name}'\n",
    "\n",
    "data_dir = './drive/My Drive/Colab Notebooks/input'\n",
    "train_dir = f'{data_dir}/train_fake.csv'\n",
    "test_dir = f'{data_dir}/test_fake.csv'\n",
    "\n",
    "out_dir = './drive/My Drive/Colab Notebooks/output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OM5OIHTTysB"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU2klIOaT7ha"
   },
   "source": [
    "## Mount google drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2l2sEh2RX9j-"
   },
   "source": [
    "Mounting Google Drive is needed to be able to access folders that are inside. After running this cell you will see a link allowing you to authorize your access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9htjq7LWQeo",
    "outputId": "fe4e2cc9-8c03-4197-c475-aa2106d0830c"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAFXdMQVYMXN"
   },
   "source": [
    "## Download model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kjBSyiQlWMNn",
    "outputId": "50c4f88f-4bc5-4c0b-c28a-517e38d98d3f"
   },
   "outputs": [],
   "source": [
    "!wget {bert_model_path}\n",
    "!unzip the file\n",
    "!unzip {bert_model_name}.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6qugUpOYnx_"
   },
   "source": [
    "## Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_WqPYNaXBYv",
    "outputId": "a995d5a5-2ebe-4968-e95c-419050882ba3"
   },
   "outputs": [],
   "source": [
    "!pip install keras_bert\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "JmaKZU00WD9i"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEmx5yddWD9j"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAMw1OeNZFFw"
   },
   "outputs": [],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "fID5q14RWD9s"
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9ITU0kiWD9s"
   },
   "outputs": [],
   "source": [
    "def display_all(df, num_rows=10000, num_cols=10000, col_width=-1):\n",
    "    with pd.option_context('display.max_rows', num_rows, 'display.max_columns', num_cols, 'display.max_colwidth', -1):\n",
    "        display(df)\n",
    "        \n",
    "def init_tokenizer_and_load_bert_model(model_dir, model_name, model_trainable=True, lowercase=True):\n",
    "\n",
    "    vocab_path = f'{model_dir}/vocab.txt'\n",
    "    config_path = f'{model_dir}/bert_config.json'\n",
    "    checkpoint_path = f'{model_dir}/bert_model.ckpt'\n",
    "    \n",
    "    tokenizer = BertTokenizer(vocab_path, do_lower_case=lowercase)\n",
    "    model = load_trained_model_from_checkpoint(config_path, checkpoint_path, trainable=model_trainable)\n",
    "    \n",
    "    print('vocab_size:', len(tokenizer.vocab))\n",
    "    print('loaded model: ', model_name)\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "def get_bert_vectors(tokenizer, df, col_name, vector_length=512):\n",
    "    tokenize = lambda sentence: tokenizer.encode_plus(sentence, max_length=vector_length, padding='max_length', truncation=True)\n",
    "    df[f'{col_name}_tokens'] = df[col_name].map(tokenize)\n",
    "\n",
    "    df[f'{col_name}_input_ids'] = df[f'{col_name}_tokens'].map(lambda x: x['input_ids'])\n",
    "    df[f'{col_name}_token_type_ids'] = df[f'{col_name}_tokens'].map(lambda x: x['token_type_ids'])\n",
    "    df[f'{col_name}_attention_mask'] = df[f'{col_name}_tokens'].map(lambda x: x['attention_mask'])\n",
    "    \n",
    "    input_ids = np.stack(df[f'{col_name}_input_ids'])\n",
    "    token_type_ids = np.stack(df[f'{col_name}_token_type_ids'])\n",
    "    attention_mask = np.stack(df[f'{col_name}_attention_mask'])\n",
    "    vectors = {'input_ids': input_ids, 'token_type_ids': token_type_ids, 'attention_mask': attention_mask}\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "def bert_predict_in_batches(vectors, num_batches, output_shape):\n",
    "\n",
    "  vector_input_ids_batches = np.array_split(vectors['input_ids'], num_batches)\n",
    "  vector_token_type_ids_batches = np.array_split(vectors['token_type_ids'], num_batches)\n",
    "  vector_attention_mask_batches = np.array_split(vectors['attention_mask'], num_batches)\n",
    "\n",
    "  X = np.array([]).reshape((0, output_shape))\n",
    "\n",
    "  input_vectors = zip(vector_input_ids_batches, vector_token_type_ids_batches, vector_attention_mask_batches)\n",
    "  for input_ids, token_type_ids, attention_mask in input_vectors:\n",
    "    all_vectors = (input_ids, token_type_ids, attention_mask)\n",
    "    predictions = bert_model.predict(all_vectors, verbose=1)\n",
    "\n",
    "    X_batch = predictions[:, 0, :]\n",
    "    print('current predictions shape: ', X_batch.shape)\n",
    "    X = np.concatenate([X, X_batch])\n",
    "    print('all predictions shape: ', X.shape)\n",
    "\n",
    "  return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ircroN0wWD91"
   },
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QBfpIHf6WD92",
    "outputId": "80d6f22b-2b93-4bc8-f86d-8d6545267c1e"
   },
   "outputs": [],
   "source": [
    "train_fake = pd.read_csv(train_dir)\n",
    "train_fake['is_fake'] = train_fake['is_fake'].astype('int8')\n",
    "test_fake = pd.read_csv(test_dir)\n",
    "\n",
    "train_fake.shape, test_fake.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvKBcS0_C8Lk"
   },
   "source": [
    "Some fields have null values - temporarily we are filing them with nans to be able to get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOUbrk1T7lTv"
   },
   "outputs": [],
   "source": [
    "train_fake.fillna('unknown', inplace=True)\n",
    "test_fake.fillna('unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeVDToVpWD95"
   },
   "source": [
    "# Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1EPmFeS4WD95",
    "outputId": "caf7c3e1-327b-4090-d147-8682d90b99bc"
   },
   "outputs": [],
   "source": [
    "tokenizer, bert_model = init_tokenizer_and_load_bert_model(model_dir, bert_model_name, model_trainable=True, lowercase=uncased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMh8y0DMWD98"
   },
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtPy55ueL3Xf",
    "outputId": "29e3cf99-458e-483c-e9b3-78cae8789af5"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_title_vectors = get_bert_vectors(tokenizer, train_fake, 'title', vector_length=512)\n",
    "[vec.shape for vec in train_title_vectors.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0nS_5uGmWD-F",
    "outputId": "5e1c8956-feb4-4e22-af71-00f98d96f709"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_text_vectors = get_bert_vectors(tokenizer, train_fake, 'text', vector_length=512)\n",
    "[vec.shape for vec in train_text_vectors.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sTEThvwh-5Ps",
    "outputId": "ec457cde-b1fc-457f-86c5-80849f5951b6"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "test_title_vectors = get_bert_vectors(tokenizer, test_fake, 'title', vector_length=512)\n",
    "[vec.shape for vec in test_title_vectors.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_q1RirB-4-4",
    "outputId": "67c341f9-a383-49c7-b99c-d5ae8845ca9d"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "test_text_vectors = get_bert_vectors(tokenizer, test_fake, 'text', vector_length=512)\n",
    "[vec.shape for vec in test_text_vectors.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEwRH4bQWD-N"
   },
   "source": [
    "## bert predict and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_output_shape = bert_model.layers[-1].output_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4WauoSt-RNO"
   },
   "outputs": [],
   "source": [
    "train_X_title = bert_predict_in_batches(train_title_vectors, 5, bert_output_shape)\n",
    "np.save(f'{out_dir}/train_X_title_{bert_model_name}.npy', train_X_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLrb6UkD-nQn"
   },
   "outputs": [],
   "source": [
    "train_X_text = bert_predict_in_batches(train_text_vectors, 15, bert_output_shape)\n",
    "np.save(f'{out_dir}/train_X_text_{bert_model_name}.npy', train_X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAkVrODw_I1-"
   },
   "outputs": [],
   "source": [
    "test_X_title = bert_predict_in_batches(test_title_vectors, 5, bert_output_shape)\n",
    "np.save(f'{out_dir}/test_X_title_{bert_model_name}.npy', test_X_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8Y2A0Sq_Ihv"
   },
   "outputs": [],
   "source": [
    "test_X_text = bert_predict_in_batches(test_text_vectors, 15, bert_output_shape)\n",
    "np.save(f'{out_dir}/test_X_text_{bert_model_name}.npy', test_X_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "modeling_bert.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}